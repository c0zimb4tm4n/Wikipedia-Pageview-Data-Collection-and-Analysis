{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Pageview Data Collection\n",
    "\n",
    "### Importing Libraries\n",
    "\n",
    "This section imports essential Python libraries required for making API requests, processing data, and visualizing results.\n",
    "\n",
    "#### Libraries:\n",
    "- **Standard Libraries**: \n",
    "  - `json`, `time`, `urllib.parse`: Used for handling JSON data, managing time delays, and encoding URLs.\n",
    "  \n",
    "- **Third-Party Libraries**:\n",
    "  - `requests`: Required to make API calls. Ensure it is installed using `pip install requests`.\n",
    "  - `pandas`: Used for data manipulation and storage in DataFrames.\n",
    "  - `datetime`: Assists with date handling and formatting.\n",
    "  - `matplotlib.pyplot`: Used for data visualization.\n",
    "\n",
    "#### Reproducibility:\n",
    "These imports ensure all necessary libraries are included for consistent execution of the code, enabling reproducibility across different environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "#\n",
    "# You will need to install these with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants for API Requests\n",
    "\n",
    "This section defines constants used to construct and manage API requests to the Wikimedia Pageviews API, ensuring consistency and reproducibility throughout the workflow.\n",
    "\n",
    "#### Key Elements:\n",
    "- **API Request URL**: The base URL (`API_REQUEST_PAGEVIEWS_ENDPOINT`) for all pageviews requests.\n",
    "- **Parameterized Request String**: The string (`API_REQUEST_PER_ARTICLE_PARAMS`) contains placeholders for project, access type, article name, and date range, which will be dynamically replaced.\n",
    "- **API Throttling**: A small delay (`API_THROTTLE_WAIT`) is added to ensure we don't exceed Wikimedia’s rate limit of 100 requests per second.\n",
    "- **Headers**: The request includes a User-Agent in `REQUEST_HEADERS` to comply with Wikimedia’s API guidelines.\n",
    "\n",
    "#### Reproducibility:\n",
    "By centralizing these values, we ensure that all requests use consistent parameters, which simplifies debugging, enhances transparency, and allows for easy modifications without altering the core code.\n",
    "#### License\n",
    "Code Owner : Dr. David W. McDonald\n",
    "\n",
    "Creative Commons CC-BY license\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making a request to the Wikimedia API they ask that you include your email address which will allow them\n",
    "# to contact you if something happens - such as - your code exceeding rate limits - or some other error \n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<trips@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. In the example, below, we only vary the article name, so the majority of the fields\n",
    "# can stay constant for each request. Of course, these values *could* be changed if necessary.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"desktop\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015070100\",   #start date\n",
    "    \"end\":         \"2024093000\"    #end date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Data Request Function\n",
    "\n",
    "The `request_pageviews_per_article` function automates the process of requesting pageview data from the Wikimedia Pageviews API. It handles API calls, parameters, and responses, ensuring consistency and reproducibility.\n",
    "\n",
    "#### Key Parameters:\n",
    "- **article_title**: Wikipedia article name (URL-encoded for special characters).\n",
    "- **access_type**: Specifies the platform (desktop, mobile-web, mobile-app).\n",
    "- **endpoint_url**: Base URL for the API.\n",
    "- **endpoint_params**: Placeholder string for request parameters.\n",
    "- **request_template**: Standard parameters for API requests (e.g., project, date range).\n",
    "- **headers**: User metadata for API compliance.\n",
    "\n",
    "#### Workflow:\n",
    "1. **Set Article Title**: Dynamically assigns the article title in the request.\n",
    "2. **Encode Title**: Handles special characters via URL encoding.\n",
    "3. **Construct API Request**: Combines endpoint and parameters for the request URL.\n",
    "4. **Make Request**: Fetches data from the API, ensuring API rate limits are respected.\n",
    "5. **Error Handling**: Catches exceptions, prints errors, and returns `None` if failed.\n",
    "\n",
    "#### Return:\n",
    "The function returns the API response in JSON format or `None` in case of errors.\n",
    "\n",
    "#### Reproducibility:\n",
    "Encapsulating the API logic ensures automated, repeatable data collection across multiple articles and access types, reducing manual steps and minimizing errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "# Function to request pageviews per article\n",
    "def request_pageviews_per_article(article_title=None, \n",
    "                                  access_type=\"desktop\",\n",
    "                                  endpoint_url=API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params=API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  request_template=ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                  headers=REQUEST_HEADERS):\n",
    "\n",
    "    # Set the article title\n",
    "    if article_title:\n",
    "        request_template['article'] = article_title\n",
    "\n",
    "    if not request_template['article']:\n",
    "        raise Exception(\"Must supply an article title to make a pageviews request.\")\n",
    "\n",
    "    # Set the access type\n",
    "    request_template['access'] = access_type\n",
    "    \n",
    "    # Encode the article title for URL\n",
    "    article_title_encoded = urllib.parse.quote(request_template['article'].replace(' ','_'))\n",
    "    request_template['article'] = article_title_encoded\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # Make the request and handle exceptions\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Article Titles from CSV\n",
    "\n",
    "This section loads Wikipedia article titles related to rare diseases from a CSV file. The `'disease'` column in the file contains the article names, which will be used in API requests.\n",
    "\n",
    "#### Steps:\n",
    "1. **Load CSV**: The CSV file is read into a DataFrame using `pandas`, ensuring a structured format.\n",
    "2. **Extract Titles**: Article titles are converted into a list, which will be iterated over for data fetching.\n",
    "\n",
    "#### Reproducibility:\n",
    "Automating the loading of article titles ensures consistent input for the workflow and reduces manual intervention, improving reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the article titles from the provided CSV file\n",
    "# The column 'disease' contains the titles of the Wikipedia articles\n",
    "df_articles = pd.read_csv(\"../data/rare-disease_cleaned.AUG.2024.csv\")\n",
    "article_titles = df_articles['disease'].tolist()  # Convert the 'disease' column into a Python list of article titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition\n",
    "\n",
    "This section automates the collection of pageview data for each article from the Wikimedia Pageviews API. We retrieve data for three access types: desktop, mobile (combined mobile-web and mobile-app), and cumulative (all-access).\n",
    "\n",
    "#### Steps:\n",
    "1. **Desktop Views**: Fetches and stores monthly desktop pageview data for each article.\n",
    "2. **Mobile Views**: Combines mobile-web and mobile-app views for each article and stores the monthly sum.\n",
    "3. **Cumulative Views**: Collects the total monthly pageviews across all access types (desktop, mobile-web, mobile-app).\n",
    "\n",
    "#### Reproducibility:\n",
    "Automating the data collection ensures consistent, repeatable results, minimizing the risk of manual errors while fetching large-scale data across multiple articles and access types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No desktop data found for Sulfadoxine/pyrimethamine\n",
      "No mobile-web or mobile-app data found for Sulfadoxine/pyrimethamine\n",
      "No all-access data found for Sulfadoxine/pyrimethamine\n",
      "No desktop data found for Cystine/glutamate transporter\n",
      "No mobile-web or mobile-app data found for Cystine/glutamate transporter\n",
      "No all-access data found for Cystine/glutamate transporter\n",
      "No desktop data found for Trimethoprim/sulfamethoxazole\n",
      "No mobile-web or mobile-app data found for Trimethoprim/sulfamethoxazole\n",
      "No all-access data found for Trimethoprim/sulfamethoxazole\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "#\n",
    "#    DATA AQUISITION\n",
    "#\n",
    "\n",
    "# Initialize empty lists to store pageview data for desktop, mobile, and cumulative views\n",
    "desktop_dict = []      # Data for desktop views will be stored here\n",
    "mobile_dict = []       # Data for combined mobile-web and mobile-app views will be stored here\n",
    "cumulative_dict = []   # Data for cumulative pageviews (all-access) will be stored here\n",
    "\n",
    "# Iterate over each article title and fetch pageview data for the different access types\n",
    "for article in article_titles:\n",
    "    # Fetch desktop pageviews for the article\n",
    "    desktop_pageviews = request_pageviews_per_article(article_title=article, access_type=\"desktop\")\n",
    "    \n",
    "    # If the API returns valid desktop data, extract the monthly view counts and store them\n",
    "    if desktop_pageviews and 'items' in desktop_pageviews:\n",
    "        for month_record in desktop_pageviews['items']:\n",
    "            # Each month is represented by a dictionary with the article title, timestamp, and views\n",
    "            desktop_month_data = {\n",
    "                \"article_title\": article,\n",
    "                \"timestamp\": month_record['timestamp'],  # Date of the views\n",
    "                \"views\": month_record['views']  # Number of views for that month\n",
    "            }\n",
    "            desktop_dict.append(desktop_month_data)  # Add the data to the desktop data list\n",
    "    else:\n",
    "        print(f\"No desktop data found for {article}\")  # Message if no desktop data is found\n",
    "\n",
    "    # Fetch mobile views by combining mobile-web and mobile-app pageviews\n",
    "    mobile_web_pageviews = request_pageviews_per_article(article_title=article, access_type=\"mobile-web\")\n",
    "    mobile_app_pageviews = request_pageviews_per_article(article_title=article, access_type=\"mobile-app\")\n",
    "    \n",
    "    # If both mobile-web and mobile-app data are available, sum their views for each month\n",
    "    if mobile_web_pageviews and 'items' in mobile_web_pageviews and mobile_app_pageviews and 'items' in mobile_app_pageviews:\n",
    "        for web_record, app_record in zip(mobile_web_pageviews['items'], mobile_app_pageviews['items']):\n",
    "            # Ensure that both mobile-web and mobile-app data refer to the same month\n",
    "            if web_record['timestamp'] == app_record['timestamp']:\n",
    "                mobile_month_data = {\n",
    "                    \"article_title\": article,\n",
    "                    \"timestamp\": web_record['timestamp'],  # Date of the views\n",
    "                    \"views\": web_record['views'] + app_record['views']  # Sum of mobile-web and mobile-app views\n",
    "                }\n",
    "                mobile_dict.append(mobile_month_data)  # Add the combined mobile data to the mobile list\n",
    "    else:\n",
    "        print(f\"No mobile-web or mobile-app data found for {article}\")  # Message if no mobile data is found\n",
    "\n",
    "    # Fetch cumulative views (all-access combines desktop, mobile-web, and mobile-app views)\n",
    "    all_access_pageviews = request_pageviews_per_article(article_title=article, access_type=\"all-access\")\n",
    "    \n",
    "    # If cumulative data (all-access) is returned, add it to the cumulative list\n",
    "    if all_access_pageviews and 'items' in all_access_pageviews:\n",
    "        for cumulative_month in all_access_pageviews['items']:\n",
    "            cumulative_month_data = {\n",
    "                \"article_title\": article,\n",
    "                \"timestamp\": cumulative_month['timestamp'],  # Date of the views\n",
    "                \"views\": cumulative_month['views']  # Total views from all access types (desktop, mobile-web, mobile-app)\n",
    "            }\n",
    "            cumulative_dict.append(cumulative_month_data)  # Add the cumulative data to the cumulative list\n",
    "    else:\n",
    "        print(f\"No all-access data found for {article}\")  # Message if no all-access data is found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing and Output\n",
    "\n",
    "This section processes the collected pageview data and saves it in a structured format for further analysis.\n",
    "\n",
    "#### Steps:\n",
    "1. **Convert Data to DataFrames**: \n",
    "   The collected pageview data (desktop, mobile, and cumulative views) is converted into `pandas` DataFrames to facilitate easy manipulation and analysis.\n",
    "   \n",
    "2. **Save Data to JSON**: \n",
    "   Each DataFrame is saved as a JSON file with clear filenames indicating the type of data (desktop, mobile, or cumulative) and the time range. The data is stored in a record-oriented format, with each observation as a separate JSON object.\n",
    "\n",
    "#### Reproducibility:\n",
    "Saving the processed data in a standard format like JSON ensures that it can be easily shared and reproduced by others, maintaining consistency and transparency in the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    DATA PROCESSING AND OUTPUT\n",
    "#\n",
    "\n",
    "# Convert the collected data into pandas DataFrames\n",
    "# Each DataFrame holds data corresponding to one of the access types: desktop, mobile, or cumulative\n",
    "df_desktop_views = pd.DataFrame(desktop_dict)  # DataFrame for desktop views\n",
    "df_mobile_views = pd.DataFrame(mobile_dict)    # DataFrame for combined mobile-web and mobile-app views\n",
    "df_all_access_views = pd.DataFrame(cumulative_dict)  # DataFrame for cumulative views (all-access)\n",
    "\n",
    "# Sort DataFrames by article title and timestamp\n",
    "df_desktop_views = df_desktop_views.sort_values(by=['article_title', 'timestamp'])\n",
    "df_mobile_views = df_mobile_views.sort_values(by=['article_title', 'timestamp'])\n",
    "df_all_access_views = df_all_access_views.sort_values(by=['article_title', 'timestamp'])\n",
    "\n",
    "# Save the DataFrames as JSON files\n",
    "# The data is saved as JSON, with each observation being an individual record (row)\n",
    "df_desktop_views.to_json(\"../data/rare-disease_monthly_desktop_201507-202409.json\", orient='records')\n",
    "df_mobile_views.to_json(\"../data/rare-disease_monthly_mobile_201507-202409.json\", orient='records')\n",
    "df_all_access_views.to_json(\"../data/rare-disease_monthly_cumulative_201507-202409.json\", orient='records')\n",
    "\n",
    "# The filenames reflect the type of data and the date range they cover"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
